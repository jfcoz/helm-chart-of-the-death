---
# Source: k8s-monitoring/charts/alloy-operator/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8s-monitoring-alloy-operator
  namespace: infra-k8s-monitoring
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
automountServiceAccountToken: true
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-monitoring-alloy-singleton
  namespace: infra-k8s-monitoring
data:
  config.alloy: |
    // Feature: Cluster Events
    declare "cluster_events" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.source.kubernetes_events "cluster_events" {
        job_name   = "integrations/kubernetes/eventhandler"
        log_format = "logfmt"
        forward_to = [loki.process.cluster_events.receiver]
      }
    
      loki.process "cluster_events" {
    
        // add a static source label to the logs so they can be differentiated / restricted if necessary
        stage.static_labels {
          values = {
            "source" = "kubernetes-events",
          }
        }
    
        // extract some of the fields from the log line, these could be used as labels, structured metadata, etc.
        stage.logfmt {
          mapping = {
            "component" = "sourcecomponent", // map the sourcecomponent field to component
            "kind" = "",
            "level" = "type", // most events don't have a level but they do have a "type" i.e. Normal, Warning, Error, etc.
            "name" = "",
            "node" = "sourcehost", // map the sourcehost field to node
            "reason" = "",
          }
        }
        // set these values as labels, they may or may not be used as index labels in Loki as they can be dropped
        // prior to being written to Loki, but this makes them available
        stage.labels {
          values = {
            "component" = "",
            "kind" = "",
            "level" = "",
            "name" = "",
            "node" = "",
            "reason" = "",
          }
        }
    
        // if kind=Node, set the node label by copying the name field
        stage.match {
          selector = "{kind=\"Node\"}"
    
          stage.labels {
            values = {
              "node" = "name",
            }
          }
        }
    
        // set the level extracted key value as a normalized log level
        stage.match {
          selector = "{level=\"Normal\"}"
    
          stage.static_labels {
            values = {
              level = "Info",
            }
          }
        }
        stage.structured_metadata {
          values = {
            "name" = "name",
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["job","level","namespace","node","source","reason"]
        }
        stage.labels {
          values = {
            "service_name" = "job",
          }
        }
        forward_to = argument.logs_destinations.value
      }
    }
    cluster_events "feature" {
      logs_destinations = [
        loki.write.locallogs.receiver,
      ]
    }
    // Self Reporting
    prometheus.exporter.unix "kubernetes_monitoring_telemetry" {
      set_collectors = ["textfile"]
      textfile {
        directory = "/etc/alloy"
      }
    }
    
    discovery.relabel "kubernetes_monitoring_telemetry" {
      targets = prometheus.exporter.unix.kubernetes_monitoring_telemetry.targets
      rule {
        target_label = "instance"
        action = "replace"
        replacement = "k8s-monitoring"
      }
      rule {
        target_label = "job"
        action = "replace"
        replacement = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      }
    }
    
    prometheus.scrape "kubernetes_monitoring_telemetry" {
      job_name   = "integrations/kubernetes/kubernetes_monitoring_telemetry"
      targets    = discovery.relabel.kubernetes_monitoring_telemetry.output
      scrape_interval = "60s"
      clustering {
        enabled = true
      }
      forward_to = [prometheus.relabel.kubernetes_monitoring_telemetry.receiver]
    }
    
    prometheus.relabel "kubernetes_monitoring_telemetry" {
      rule {
        source_labels = ["__name__"]
        regex = "grafana_kubernetes_monitoring_.*"
        action = "keep"
      }
      forward_to = [
        prometheus.remote_write.localprometheus.receiver,
      ]
    }
    
    
    
    
    // Destination: localLogs (loki)
    otelcol.exporter.loki "locallogs" {
      forward_to = [loki.write.locallogs.receiver]
    }
    
    loki.write "locallogs" {
      endpoint {
        url = "http://loki-distributor.infra-loki:3100/loki/api/v1/push"
        retry_on_http_429 = true
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "change-me",
        "k8s_cluster_name" = "change-me",
      }
    }
    // Destination: localPrometheus (prometheus)
    otelcol.exporter.prometheus "localprometheus" {
      add_metric_suffixes = true
      resource_to_telemetry_conversion = false
      forward_to = [prometheus.remote_write.localprometheus.receiver]
    }
    
    prometheus.remote_write "localprometheus" {
      endpoint {
        url = "http://prometheus-kube-prometheus-prometheus.infra-prometheus:9090"
        headers = {
        }
        tls_config {
          insecure_skip_verify = false
        }
        send_native_histograms = false
    
        queue_config {
          capacity = 10000
          min_shards = 1
          max_shards = 50
          max_samples_per_send = 2000
          batch_send_deadline = "5s"
          min_backoff = "30ms"
          max_backoff = "5s"
          retry_on_http_429 = true
          sample_age_limit = "0s"
        }
    
        write_relabel_config {
          source_labels = ["cluster"]
          regex = ""
          replacement = "change-me"
          target_label = "cluster"
        }
        write_relabel_config {
          source_labels = ["k8s_cluster_name"]
          regex = ""
          replacement = "change-me"
          target_label = "k8s_cluster_name"
        }
      }
    
      wal {
        truncate_frequency = "2h"
        min_keepalive_time = "5m"
        max_keepalive_time = "8h"
      }
    }
  self-reporting-metric.prom: |
    # HELP grafana_kubernetes_monitoring_build_info A metric to report the version of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_build_info gauge
    grafana_kubernetes_monitoring_build_info{version="3.8.1", namespace="infra-k8s-monitoring"} 1
    # HELP grafana_kubernetes_monitoring_feature_info A metric to report the enabled features of the Kubernetes Monitoring Helm chart
    # TYPE grafana_kubernetes_monitoring_feature_info gauge
    grafana_kubernetes_monitoring_feature_info{feature="clusterEvents", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="nodeLogs", version="1.0.0"} 1
    grafana_kubernetes_monitoring_feature_info{feature="podLogs", method="volumes", version="1.0.0"} 1
    # EOF
---
# Source: k8s-monitoring/templates/alloy-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: k8s-monitoring-alloy-logs
  namespace: infra-k8s-monitoring
data:
  config.alloy: |
    // Feature: Node Logs
    declare "node_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      loki.relabel "journal" {
    
        // copy all journal labels and make the available to the pipeline stages as labels, there is a label
        // keep defined to filter out unwanted labels, these pipeline labels can be set as structured metadata
        // as well, the following labels are available:
        // - boot_id
        // - cap_effective
        // - cmdline
        // - comm
        // - exe
        // - gid
        // - hostname
        // - machine_id
        // - pid
        // - stream_id
        // - systemd_cgroup
        // - systemd_invocation_id
        // - systemd_slice
        // - systemd_unit
        // - transport
        // - uid
        //
        // More Info: https://www.freedesktop.org/software/systemd/man/systemd.journal-fields.html
        rule {
          action = "labelmap"
          regex = "__journal__(.+)"
        }
    
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "unit"
        }
    
        // the service_name label will be set automatically in loki if not set, and the unit label
        // will not allow service_name to be set automatically.
        rule {
          action = "replace"
          source_labels = ["__journal__systemd_unit"]
          replacement = "$1"
          target_label = "service_name"
        }
    
        forward_to = [] // No forward_to is used in this component, the defined rules are used in the loki.source.journal component
      }
    
      loki.source.journal "worker" {
        path = "/var/log/journal"
        format_as_json = false
        max_age = "8h"
        relabel_rules = loki.relabel.journal.rules
        labels = {
          job = "integrations/kubernetes/journal",
          instance = sys.env("HOSTNAME"),
        }
        forward_to = [loki.process.journal_logs.receiver]
      }
    
      loki.process "journal_logs" {
        stage.static_labels {
          values = {
            // add a static source label to the logs so they can be differentiated / restricted if necessary
            "source" = "journal",
            // default level to unknown
            level = "unknown",
          }
        }
    
        // Attempt to determine the log level, most k8s workers are either in logfmt or klog formats
        // check to see if the log line matches the klog format (https://github.com/kubernetes/klog)
        stage.match {
          // unescaped regex: ([IWED][0-9]{4}\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\.[0-9]+)
          selector = "{level=\"unknown\"} |~ \"([IWED][0-9]{4}\\\\s+[0-9]{2}:[0-9]{2}:[0-9]{2}\\\\.[0-9]+)\""
    
          // extract log level, klog uses a single letter code for the level followed by the month and day i.e. I0119
          stage.regex {
            expression = "((?P<level>[A-Z])[0-9])"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(I)"
            replace = "INFO"
          }
    
          // if the extracted level is W set WARN
          stage.replace {
            source = "level"
            expression = "(W)"
            replace = "WARN"
          }
    
          // if the extracted level is E set ERROR
          stage.replace {
            source = "level"
            expression = "(E)"
            replace = "ERROR"
          }
    
          // if the extracted level is I set INFO
          stage.replace {
            source = "level"
            expression = "(D)"
            replace = "DEBUG"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // if the level is still unknown, do one last attempt at detecting it based on common levels
        stage.match {
          selector = "{level=\"unknown\"}"
    
          // unescaped regex: (?i)(?:"(?:level|loglevel|levelname|lvl|levelText|SeverityText)":\s*"|\s*(?:level|loglevel|levelText|lvl)="?|\s+\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))("|\s+|-|\s*\])
          stage.regex {
            expression = "(?i)(?:\"(?:level|loglevel|levelname|lvl|levelText|SeverityText)\":\\s*\"|\\s*(?:level|loglevel|levelText|lvl)=\"?|\\s+\\[?)(?P<level>(DEBUG?|DBG|INFO?(RMATION)?|WA?RN(ING)?|ERR(OR)?|CRI?T(ICAL)?|FATAL|FTL|NOTICE|TRACE|TRC|PANIC|PNC|ALERT|EMERGENCY))(\"|\\s+|-|\\s*\\])"
          }
    
          // set the extracted level to be a label
          stage.labels {
            values = {
              level = "",
            }
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["__tenant_id__","instance","job","level","name","unit","service_name","source"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    node_logs "feature" {
      logs_destinations = [
        loki.write.locallogs.receiver,
      ]
    }
    // Feature: Pod Logs
    declare "pod_logs" {
      argument "logs_destinations" {
        comment = "Must be a list of log destinations where collected logs should be forwarded to"
      }
    
      discovery.relabel "filtered_pods" {
        targets = discovery.kubernetes.pods.targets
        rule {
          source_labels = ["__meta_kubernetes_namespace"]
          action = "replace"
          target_label = "namespace"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_name"]
          action = "replace"
          target_label = "pod"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_container_name"]
          action = "replace"
          target_label = "container"
        }
        rule {
          source_labels = ["__meta_kubernetes_namespace", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "$1"
          target_label = "job"
        }
    
        // set the container runtime as a label
        rule {
          action = "replace"
          source_labels = ["__meta_kubernetes_pod_container_id"]
          regex = "^(\\S+):\\/\\/.+$"
          replacement = "$1"
          target_label = "tmp_container_runtime"
        }
    
        // make all labels on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_label_(.+)"
        }
    
        // make all annotations on the pod available to the pipeline as labels,
        // they are omitted before write to loki via stage.label_keep unless explicitly set
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_(.+)"
        }
    
        // explicitly set service_name. if not set, loki will automatically try to populate a default.
        // see https://grafana.com/docs/loki/latest/get-started/labels/#default-labels-for-all-users
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.name]
        // - pod.label[app.kubernetes.io/name]
        // - k8s.container.name
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_name",
            "__meta_kubernetes_pod_label_app_kubernetes_io_name",
            "__meta_kubernetes_pod_container_name",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_name"
        }
    
        // explicitly set service_namespace.
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.namespace]
        // - pod.namespace
        rule {
          action = "replace"
          source_labels = [
            "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_namespace",
            "namespace",
          ]
          separator = ";"
          regex = "^(?:;*)?([^;]+).*$"
          replacement = "$1"
          target_label = "service_namespace"
        }
    
        // explicitly set service_instance_id.
        //
        // choose the first value found from the following ordered list:
        // - pod.annotation[resource.opentelemetry.io/service.instance.id]
        // - concat([k8s.namespace.name, k8s.pod.name, k8s.container.name], '.')
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_resource_opentelemetry_io_service_instance_id"]
          target_label = "service_instance_id"
        }
        rule {
          source_labels = ["service_instance_id", "namespace", "pod", "container"]
          separator = "."
          regex = "^\\.([^.]+\\.[^.]+\\.[^.]+)$"
          target_label = "service_instance_id"
        }
    
        // set resource attributes
        rule {
          action = "labelmap"
          regex = "__meta_kubernetes_pod_annotation_resource_opentelemetry_io_(.+)"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_annotation_k8s_grafana_com_logs_job"]
          regex = "(.+)"
          target_label = "job"
        }
        rule {
          source_labels = ["__meta_kubernetes_pod_label_app_kubernetes_io_name"]
          regex = "(.+)"
          target_label = "app_kubernetes_io_name"
        }
      }
    
      discovery.kubernetes "pods" {
        role = "pod"
        selectors {
          role = "pod"
          field = "spec.nodeName=" + sys.env("HOSTNAME")
        }
      }
    
      discovery.relabel "filtered_pods_with_paths" {
        targets = discovery.relabel.filtered_pods.output
    
        rule {
          source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
          separator = "/"
          action = "replace"
          replacement = "/var/log/pods/*$1/*.log"
          target_label = "__path__"
        }
      }
    
      local.file_match "pod_logs" {
        path_targets = discovery.relabel.filtered_pods_with_paths.output
      }
    
      loki.source.file "pod_logs" {
        targets    = local.file_match.pod_logs.targets
        forward_to = [loki.process.pod_logs.receiver]
      }
    
      loki.process "pod_logs" {
        stage.match {
          selector = "{tmp_container_runtime=~\"containerd|cri-o|\"}"
          // the cri processing stage extracts the following k/v pairs: log, stream, time, flags
          stage.cri {
            max_partial_lines = 100
          }
    
          // Set the extract flags and stream values as labels
          stage.labels {
            values = {
              flags  = "",
              stream  = "",
            }
          }
        }
    
        stage.match {
          selector = "{tmp_container_runtime=\"docker\"}"
          // the docker processing stage extracts the following k/v pairs: log, stream, time
          stage.docker {}
    
          // Set the extract stream value as a label
          stage.labels {
            values = {
              stream  = "",
            }
          }
        }
    
        // Drop the filename label, since it's not really useful in the context of Kubernetes, where we already have cluster,
        // namespace, pod, and container labels. Drop any structured metadata. Also drop the temporary
        // container runtime label as it is no longer needed.
        stage.label_drop {
          values = [
            "filename",
            "tmp_container_runtime",
          ]
        }
        stage.structured_metadata {
          values = {
            "k8s_pod_name" = "k8s_pod_name",
            "pod" = "pod",
            "service_instance_id" = "service_instance_id",
          }
        }
    
        // Only keep the labels that are defined in the `keepLabels` list.
        stage.label_keep {
          values = ["__tenant_id__","app_kubernetes_io_name","container","instance","job","level","namespace","service_name","service_namespace","deployment_environment","deployment_environment_name","k8s_namespace_name","k8s_deployment_name","k8s_statefulset_name","k8s_daemonset_name","k8s_cronjob_name","k8s_job_name","k8s_node_name"]
        }
    
        forward_to = argument.logs_destinations.value
      }
    }
    pod_logs "feature" {
      logs_destinations = [
        loki.write.locallogs.receiver,
      ]
    }
    
    
    
    
    // Destination: localLogs (loki)
    otelcol.exporter.loki "locallogs" {
      forward_to = [loki.write.locallogs.receiver]
    }
    
    loki.write "locallogs" {
      endpoint {
        url = "http://loki-distributor.infra-loki:3100/loki/api/v1/push"
        retry_on_http_429 = true
        tls_config {
          insecure_skip_verify = false
        }
        min_backoff_period = "500ms"
        max_backoff_period = "5m"
        max_backoff_retries = "10"
      }
      external_labels = {
        "cluster" = "change-me",
        "k8s_cluster_name" = "change-me",
      }
    }
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8s-monitoring-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - collectors.grafana.com
    resources:
      - alloys
      - alloys/status
      - alloys/finalizers
    verbs:
      - create
      - delete
      - get
      - list
      - patch
      - update
      - watch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: k8s-monitoring-alloy-operator
rules:
  # Rules which allow the management of ConfigMaps, ServiceAccounts, and Services.
  - apiGroups: [""]
    resources: ["configmaps", "secrets", "serviceaccounts", "services"]
    verbs: ["*"]
  # Rules which allow the management of DaemonSets, Deployments, and StatefulSets.
  - apiGroups: ["apps"]
    resources: ["daemonsets", "deployments", "statefulsets"]
    verbs: ["*"]
  # Rules which allow the management of Horizontal Pod Autoscalers.
  - apiGroups: ["autoscaling"]
    resources: ["horizontalpodautoscalers"]
    verbs: ["*"]
  # Rules which allow the management of Ingresses and NetworkPolicies.
  - apiGroups: ["networking.k8s.io"]
    resources: ["ingresses", "networkpolicies"]
    verbs: ["*"]
  # Rules which allow the management of PodDisruptionBudgets.
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["*"]
  # Rules which allow the management of ClusterRoles, ClusterRoleBindings, Roles, and RoleBindings.
  - apiGroups: ["rbac.authorization.k8s.io"]
    resources: ["clusterroles", "clusterrolebindings", "roles", "rolebindings"]
    verbs: ["*"]
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-manager.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8s-monitoring-alloy-operator-alloy-manager
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8s-monitoring-alloy-operator-alloy-manager
subjects:
  - kind: ServiceAccount
    name: k8s-monitoring-alloy-operator
    namespace: infra-k8s-monitoring
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/alloy-objects.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: k8s-monitoring-alloy-operator
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: k8s-monitoring-alloy-operator
subjects:
  - kind: ServiceAccount
    name: k8s-monitoring-alloy-operator
    namespace: infra-k8s-monitoring
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8s-monitoring-alloy-operator-leader-election-role
  namespace: infra-k8s-monitoring
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
rules:
  - apiGroups:
      - ""
    resources:
      - configmaps
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - coordination.k8s.io
    resources:
      - leases
    verbs:
      - get
      - list
      - watch
      - create
      - update
      - patch
      - delete
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
      - patch
---
# Source: k8s-monitoring/charts/alloy-operator/templates/rbac/leader-election.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8s-monitoring-alloy-operator-leader-election-rolebinding
  namespace: infra-k8s-monitoring
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: k8s-monitoring-alloy-operator-leader-election-role
subjects:
  - kind: ServiceAccount
    name: k8s-monitoring-alloy-operator
    namespace: infra-k8s-monitoring
---
# Source: k8s-monitoring/charts/alloy-operator/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: k8s-monitoring-alloy-operator
  namespace: infra-k8s-monitoring
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  type: ClusterIP
  ports:
    - name: http
      port: 8081
      targetPort: http
      protocol: TCP
    - name: metrics
      port: 8082
      targetPort: metrics
      protocol: TCP
  selector:
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
---
# Source: k8s-monitoring/charts/alloy-operator/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: k8s-monitoring-alloy-operator
  namespace: infra-k8s-monitoring
  labels:
    helm.sh/chart: alloy-operator-0.4.2
    app.kubernetes.io/name: alloy-operator
    app.kubernetes.io/instance: k8s-monitoring
    app.kubernetes.io/version: "1.6.0"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: alloy-operator
      app.kubernetes.io/instance: k8s-monitoring
  template:
    metadata:
      labels:
        helm.sh/chart: alloy-operator-0.4.2
        app.kubernetes.io/name: alloy-operator
        app.kubernetes.io/instance: k8s-monitoring
        app.kubernetes.io/version: "1.6.0"
        app.kubernetes.io/managed-by: Helm
    spec:
      serviceAccountName: k8s-monitoring-alloy-operator
      securityContext:
        runAsNonRoot: true
      containers:
        - name: alloy-operator
          image: "ghcr.io/grafana/alloy-operator:1.6.0"
          imagePullPolicy: IfNotPresent
          args:
            - --health-probe-bind-address=:8081
            - --metrics-bind-address=:8082
            - --leader-elect
            - --leader-election-id=k8s-monitoring-alloy-operator

          ports:
            - name: http
              containerPort: 8081
              protocol: TCP
            - name: metrics
              containerPort: 8082
              protocol: TCP
          livenessProbe:
            httpGet:
              path: /healthz
              port: 8081
            initialDelaySeconds: 15
            periodSeconds: 20
          readinessProbe:
            httpGet:
              path: /readyz
              port: 8081
            initialDelaySeconds: 5
            periodSeconds: 10
          resources:
            limits: {}
            requests: {}
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
      nodeSelector:
        kubernetes.io/os: linux
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8s-monitoring-alloy-singleton
  namespace: infra-k8s-monitoring
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableHttpServerPort: true
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    initialDelaySeconds: 10
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: false
      extra: []
      varlog: false
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add: []
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    timeoutSeconds: 1
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    extraLabels: {}
    hostNetwork: false
    hostPID: false
    initContainers: []
    minReadySeconds: 10
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations: []
    topologySpreadConstraints: []
    type: deployment
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-singleton
  networkPolicy:
    egress:
    - {}
    enabled: false
    flavor: kubernetes
    ingress:
    - {}
    policyTypes:
    - Ingress
    - Egress
  rbac:
    clusterRules:
    - apiGroups:
      - ""
      resources:
      - nodes
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/pods
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/metrics
      verbs:
      - get
      - list
      - watch
    - nonResourceURLs:
      - /metrics
      verbs:
      - get
    create: true
    namespaces: []
    rules:
    - apiGroups:
      - ""
      - discovery.k8s.io
      - networking.k8s.io
      resources:
      - endpoints
      - endpointslices
      - ingresses
      - pods
      - services
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - pods
      - pods/log
      - namespaces
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.grafana.com
      resources:
      - podlogs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - prometheusrules
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - alertmanagerconfigs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - events
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - configmaps
      - secrets
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - apps
      - extensions
      resources:
      - replicasets
      verbs:
      - get
      - list
      - watch
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/alloy.yaml
apiVersion: collectors.grafana.com/v1alpha1
kind: Alloy
metadata:
  name: k8s-monitoring-alloy-logs
  namespace: infra-k8s-monitoring
  annotations:
    helm.sdk.operatorframework.io/uninstall-wait: "true"
spec:
  alloy:
    clustering:
      enabled: false
      name: ""
      portName: http
    configMap:
      content: ""
      create: false
      key: null
      name: null
    enableHttpServerPort: true
    enableReporting: true
    envFrom: []
    extraArgs: []
    extraEnv: []
    extraPorts: []
    hostAliases: []
    initialDelaySeconds: 10
    lifecycle: {}
    listenAddr: 0.0.0.0
    listenPort: 12345
    listenScheme: HTTP
    livenessProbe: {}
    mounts:
      dockercontainers: true
      extra: []
      varlog: true
    resources: {}
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        add: []
        drop:
        - ALL
      seccompProfile:
        type: RuntimeDefault
    stabilityLevel: generally-available
    storagePath: /tmp/alloy
    timeoutSeconds: 1
    uiPathPrefix: /
  configReloader:
    customArgs: []
    enabled: true
    image:
      digest: ""
      registry: quay.io
      repository: prometheus-operator/prometheus-config-reloader
      tag: v0.81.0
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
    securityContext: {}
  controller:
    affinity: {}
    autoscaling:
      enabled: false
      horizontal:
        enabled: false
        maxReplicas: 5
        minReplicas: 1
        scaleDown:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 300
        scaleUp:
          policies: []
          selectPolicy: Max
          stabilizationWindowSeconds: 0
        targetCPUUtilizationPercentage: 0
        targetMemoryUtilizationPercentage: 80
      maxReplicas: 5
      minReplicas: 1
      scaleDown:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 300
      scaleUp:
        policies: []
        selectPolicy: Max
        stabilizationWindowSeconds: 0
      targetCPUUtilizationPercentage: 0
      targetMemoryUtilizationPercentage: 80
      vertical:
        enabled: false
        recommenders: []
        resourcePolicy:
          containerPolicies:
          - containerName: alloy
            controlledResources:
            - cpu
            - memory
            controlledValues: RequestsAndLimits
            maxAllowed: {}
            minAllowed: {}
        updatePolicy: null
    dnsPolicy: ClusterFirst
    enableStatefulSetAutoDeletePVC: false
    extraAnnotations: {}
    extraContainers: []
    extraLabels: {}
    hostNetwork: false
    hostPID: false
    initContainers: []
    minReadySeconds: 10
    nodeSelector:
      kubernetes.io/os: linux
    parallelRollout: true
    podAnnotations:
      k8s.grafana.com/logs.job: integrations/alloy
    podDisruptionBudget:
      enabled: false
      maxUnavailable: null
      minAvailable: null
    podLabels: {}
    priorityClassName: ""
    replicas: 1
    terminationGracePeriodSeconds: null
    tolerations:
    - effect: NoSchedule
      operator: Exists
    topologySpreadConstraints: []
    type: daemonset
    updateStrategy: {}
    volumeClaimTemplates: []
    volumes:
      extra: []
  crds:
    create: false
  extraObjects: []
  global:
    image:
      pullSecrets: []
      registry: ""
    podSecurityContext: {}
  image:
    digest: null
    pullPolicy: IfNotPresent
    pullSecrets: []
    registry: docker.io
    repository: grafana/alloy
    tag: null
  ingress:
    annotations: {}
    enabled: false
    extraPaths: []
    faroPort: 12347
    hosts:
    - chart-example.local
    labels: {}
    path: /
    pathType: Prefix
    tls: []
  nameOverride: alloy-logs
  networkPolicy:
    egress:
    - {}
    enabled: false
    flavor: kubernetes
    ingress:
    - {}
    policyTypes:
    - Ingress
    - Egress
  rbac:
    clusterRules:
    - apiGroups:
      - ""
      resources:
      - nodes
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/pods
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - nodes/metrics
      verbs:
      - get
      - list
      - watch
    - nonResourceURLs:
      - /metrics
      verbs:
      - get
    create: true
    namespaces: []
    rules:
    - apiGroups:
      - ""
      - discovery.k8s.io
      - networking.k8s.io
      resources:
      - endpoints
      - endpointslices
      - ingresses
      - pods
      - services
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - pods
      - pods/log
      - namespaces
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.grafana.com
      resources:
      - podlogs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - prometheusrules
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - alertmanagerconfigs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - monitoring.coreos.com
      resources:
      - podmonitors
      - servicemonitors
      - probes
      - scrapeconfigs
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - events
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - ""
      resources:
      - configmaps
      - secrets
      verbs:
      - get
      - list
      - watch
    - apiGroups:
      - apps
      - extensions
      resources:
      - replicasets
      verbs:
      - get
      - list
      - watch
  service:
    annotations: {}
    clusterIP: ""
    enabled: true
    internalTrafficPolicy: Cluster
    nodePort: 31128
    type: ClusterIP
  serviceAccount:
    additionalLabels: {}
    annotations: {}
    automountServiceAccountToken: true
    create: true
    name: null
  serviceMonitor:
    additionalLabels: {}
    enabled: false
    interval: ""
    metricRelabelings: []
    relabelings: []
    tlsConfig: {}
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8s-monitoring-add-finalizer
  namespace: infra-k8s-monitoring
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: k8s-monitoring-remove-alloy-and-finalizer
  namespace: infra-k8s-monitoring
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8s-monitoring-add-finalizer
  namespace: infra-k8s-monitoring
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: k8s-monitoring-remove-alloy-and-finalizer
  namespace: infra-k8s-monitoring
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
rules:
  - apiGroups: ["apps"]
    resources: ["deployments"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["collectors.grafana.com"]
    resources: ["alloys"]
    verbs: ["get", "list", "watch", "delete"]
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8s-monitoring-add-finalizer
  namespace: infra-k8s-monitoring
  annotations:
    "helm.sh/hook": post-install,post-upgrade
    "helm.sh/hook-delete-policy": before-hook-creation,hook-succeeded
    "helm.sh/hook-weight": "5"
subjects:
  - kind: ServiceAccount
    name: k8s-monitoring-add-finalizer
    namespace: infra-k8s-monitoring
roleRef:
  kind: Role
  name: k8s-monitoring-add-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: k8s-monitoring-remove-alloy-and-finalizer
  namespace: infra-k8s-monitoring
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
subjects:
  - kind: ServiceAccount
    name: k8s-monitoring-remove-alloy-and-finalizer
    namespace: infra-k8s-monitoring
roleRef:
  kind: Role
  name: k8s-monitoring-remove-alloy-and-finalizer
  apiGroup: rbac.authorization.k8s.io
---
# Source: k8s-monitoring/templates/hooks/post-install_add-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8s-monitoring-add-finalizer
  namespace: infra-k8s-monitoring
  labels:
    app.kubernetes.io/name: k8s-monitoring-add-finalizer
    app.kubernetes.io/instance: k8s-monitoring
    helm.sh/chart: k8s-monitoring-3.8.1
  annotations:
    helm.sh/hook: post-install,post-upgrade
    helm.sh/hook-weight: "15"
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8s-monitoring-add-finalizer
      labels:
        app.kubernetes.io/name: k8s-monitoring
        app.kubernetes.io/instance: k8s-monitoring
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8s-monitoring-add-finalizer
      containers:
        - name: add-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              kubectl patch \
                --namespace=infra-k8s-monitoring \
                --patch='{"metadata":{"finalizers":["k8s.grafana.com/finalizer"]}}' \
                deployment/k8s-monitoring-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
---
# Source: k8s-monitoring/templates/hooks/pre-delete_remove-alloy-and-finalizer.yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: k8s-monitoring-remove-alloy-and-finalizer
  namespace: infra-k8s-monitoring
  labels:
    app.kubernetes.io/name: k8s-monitoring-remove-alloy-and-finalizer
    app.kubernetes.io/instance: k8s-monitoring
    helm.sh/chart: k8s-monitoring-3.8.1
  annotations:
    helm.sh/hook: pre-delete
    helm.sh/hook-delete-policy: before-hook-creation,hook-succeeded
spec:
  ttlSecondsAfterFinished: 300
  backoffLimit: 3
  template:
    metadata:
      name: k8s-monitoring-remove-alloy-and-finalizer
      labels:
        app.kubernetes.io/name: k8s-monitoring
        app.kubernetes.io/instance: k8s-monitoring
        linkerd.io/inject: disabled
        sidecar.istio.io/inject: "false"
    spec:
      restartPolicy: Never
      nodeSelector:
        kubernetes.io/os: linux
      serviceAccountName: k8s-monitoring-remove-alloy-and-finalizer
      containers:
        - name: remove-finalizers
          image: "ghcr.io/grafana/helm-chart-toolbox-kubectl:0.1.2"
          imagePullPolicy: IfNotPresent
          command:
            - /bin/bash
            - -ce
            - |
              echo "Deleting Alloy instance: alloy/k8s-monitoring-alloy-singleton..."
              kubectl delete alloy/k8s-monitoring-alloy-singleton --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8s-monitoring-alloy-singleton --timeout=60s || echo "Timed out waiting for deletion of alloy/k8s-monitoring-alloy-singleton or it may not exist."

              echo "Deleting Alloy instance: alloy/k8s-monitoring-alloy-logs..."
              kubectl delete alloy/k8s-monitoring-alloy-logs --ignore-not-found=true --wait
              kubectl wait --for=delete alloy/k8s-monitoring-alloy-logs --timeout=60s || echo "Timed out waiting for deletion of alloy/k8s-monitoring-alloy-logs or it may not exist."

              kubectl patch \
                --namespace=infra-k8s-monitoring \
                --type json \
                --patch='[{"op": "remove", "path": "/metadata/finalizers"}]' \
                deployment/k8s-monitoring-alloy-operator
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsNonRoot: true
            runAsUser: 4242
            seccompProfile:
              type: RuntimeDefault
